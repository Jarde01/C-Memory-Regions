{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch-transformer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jarde01/C-Memory-Regions/blob/master/Pytorch_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq1EsqzSmpOg",
        "colab_type": "code",
        "outputId": "ff87f278-f6fc-45ab-e13e-4cb3d03adf32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip install pytorch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/67/f403d4ae6e9cd74b546ee88cccdb29b8415a9c1b3d80aebeb20c9ea91d96/pytorch-1.0.2.tar.gz\n",
            "Building wheels for collected packages: pytorch\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "Installing collected packages: pytorch\n",
            "  Running setup.py install for pytorch ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command \"/usr/bin/python3 -u -c 'import setuptools, tokenize;__file__='\"'\"'/tmp/pip-install-099rr7ht/pytorch/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-is97x7_h/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-install-099rr7ht/pytorch/\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrJQAYuiyGZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pytorch Transformer\n",
        "### From https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0ZU81-lueR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi-ygWT1ZxDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Embedder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, d_model)\n",
        "  def forward(self, x):\n",
        "    return self.embed(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfzBbVfbmr1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "  def __init__(self, d_model, max_seq_len = 50):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    # positional encoding matrix\n",
        "    pe = torch.zeros(max_seq_len, d_model)\n",
        "    for pos in range(max_seq_len):\n",
        "      for i in range(0, d_model, 2):\n",
        "        pe[pos, i]= math.sin(pos/10000 ** ((2*i)/d_model))\n",
        "        pe[pos, i+1] = math.cos(pos/ (10000 ** ((2 *( i + 1))/d_model)))\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe', pe)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # making embeddings larger1\n",
        "    x = x*math.sqrt(self.d_model)\n",
        "\n",
        "    # constant to embedding\n",
        "    seq_len = x.size(1)\n",
        "    x = x + Variable(self.pe[:,:seq_len], requires_grad=False).cuda()\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0IUp7c10Acm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "4c1c19bc-3bb1-4bc0-c0e1-d3dec0960dfb"
      },
      "source": [
        "batch = next(iter(train_iter))\n",
        "input_seq = batch.English.transpose(0,1)\n",
        "input_pad = EN_TEXT.vocab.stoi['<pad>']\n",
        "\n",
        "# Creating a mask with 0 whenever padding in input\n",
        "input_msk = (input_seq != input_pad).unsqueeze(1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e8553e6127c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnglish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEN_TEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Creating a mask with 0 whenever padding in input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_iter' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_sXajGinxz4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0qKeSxTnXZ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "2404e914-fda3-4f4b-ba9f-fcdf8bc63452"
      },
      "source": [
        "target_seq = batch.French.transpose(0,1)\n",
        "target_pad = FR_TEXT.vocab.stoi['<pad>']\n",
        "target_mask = (target_seq != target_pad).unsqueeze(1)\n",
        "\n",
        "size = target_seq.size(1)\n",
        "nopeak_mask = np.triu(np.ones(1,size,size), k=1).astype('uint8')\n",
        "nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n",
        "\n",
        "target_msk = target_msk & nopeak_msk"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7e16701ce8dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFrench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtarget_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFR_TEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_seq\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvjF-IuxoGj7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "631d73a6-6b01-465c-852c-5f3bba946d16"
      },
      "source": [
        "nopeak_mask = np.triu(np.ones((1,10,10)),k=1).astype('uint8')\n",
        "nopeak_mask = torch.from_numpy(subsequent_mask) == 0\n",
        "print(nopeak_mask)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4b0f42f53438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnopeak_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnopeak_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsequent_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnopeak_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'subsequent_mask' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpc5_p_Qucmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, heads, d_model, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.d_k = d_model //heads\n",
        "    self.h = heads\n",
        "    \n",
        "    self.q_linear = nn.Linear(d_model, d_model)\n",
        "    self.v_linear = nn.Linear(d_model, d_model)\n",
        "    self.k_linear = nn.Linear(d_model, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    bs = q.size(10)\n",
        "    \n",
        "    # linear ops and splitting onto heads\n",
        "    k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "    q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "    v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "\n",
        "    # transport --> bs * h * s1 * d_model\n",
        "    k = k.transpose(1,2)\n",
        "    q = q.transpose(1,2)\n",
        "    v = v.transpose(1,2)\n",
        "    \n",
        "    # calculation of attention\n",
        "    scores = attention(q,k,v, self.d_k, mask, self.dropout)\n",
        "    \n",
        "    # concat heads\n",
        "    concat = scores.transpose(1,2).contiguous().view(bs,-1, self.d_model)\n",
        "    \n",
        "    output = self.out(concat)\n",
        "    return output\n",
        "  \n",
        "  def attention(q,k,v,d_k,mask=None,dropout=None):\n",
        "    print(f\"k: {k.shape}\")\n",
        "    print(f\"k.transpose(-2,-1): {k.transpose(-2,-1)}\")\n",
        "    scores = torch.matmul(q,k.transpose(-2,-1)) / math.sqrt(d_k)\n",
        "    \n",
        "    if mask is not None:\n",
        "      mask = mask.unsqueeze(1)\n",
        "      print(f\"scores before mask: {scores}\")\n",
        "      scores = scores.masked_fill(mask==0, -1e9)\n",
        "      \n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    print(f\"scores after mask: {scores}\")\n",
        "    \n",
        "    if dropout is not None:\n",
        "      scores = dropout(scores)\n",
        "      \n",
        "    output = torch.matmul(scores, v)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vewa01fp4iEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
        "    super().__init__() \n",
        "    # We set d_ff as a default to 2048\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    \n",
        "  def feedforward(self,x):\n",
        "    x = self.dropout(F.relu(self.liniear_1(x)))\n",
        "    x = self.linear_2(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETuM4uaz6nYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Normalize(nn.Module):\n",
        "  def __init__(self, d_model, epsilon = 1e-6):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.size = d_model\n",
        "    \n",
        "    # learnable params for normalization calibration\n",
        "    self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "    self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "    self.epsilon = epsilon\n",
        "    \n",
        "  def forward(self, x):\n",
        "    numer = x-x.mean(dim=1, keepdim=True)\n",
        "    denom = x.std(dim=-1, keepdim=True) + self.epsilon\n",
        "    \n",
        "    norm = self.alpha * numer / denom + self.bias\n",
        "    return norm\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTjoEJf-IobW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBL4cbg5Iol8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, heads, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.norm_1 = Norm(d_model)\n",
        "    self.norm_2 = Norm(d_model)\n",
        "    self.norm_3 = Norm(d_model)\n",
        "\n",
        "    self.dropout_1 = nn.Dropout(dropout)\n",
        "    self.dropout_2 = nn.Dropout(dropout)\n",
        "    self.dropout_3 = nn.Dropout(dropout)\n",
        "\n",
        "    self.attn_1 = MultiHeadAttention(heads, d_model)\n",
        "    self.attn_2 = MultiHeadAttention(heads, d_model)\n",
        "    self.ff = FeedForward(d_model).cuda()\n",
        "      \n",
        "  def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "    x2 = self.norm_1(x)\n",
        "    x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "    x2 = self.norm_2(x)\n",
        "    x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
        "    src_mask))\n",
        "    x2 = self.norm_3(x)\n",
        "    x = x + self.dropout_3(self.ff(x2))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-MYbZb5I7sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDirK_o07nvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encooder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, N, head):\n",
        "    super().__init__()\n",
        "    self.N = N\n",
        "    self.embed = Embedder(vocab_size, d_model)\n",
        "    self.pe = PositionalEncoder(d_model)\n",
        "    self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
        "    self.norm = Normalize(d_model)\n",
        "    \n",
        "  def forward(self, src, mask):\n",
        "    x = self.embed(src)\n",
        "    x = self.pe(x)\n",
        "    for i in range(N):\n",
        "      x = self.layers[i](x, mask)\n",
        "    return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCMDh7esH0Ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, N, heads):\n",
        "    super().__init__()\n",
        "    self.N = N\n",
        "    self.embed - Embedder(vocab_size, d_model)\n",
        "    self.pe = PositionalEncoder(d_model)\n",
        "    self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
        "    self.norm = Normalize(d_model)\n",
        "    \n",
        "  def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "    x = self.embed(trg)\n",
        "    x = self.pe(x)\n",
        "    for i in range(self.N):\n",
        "      x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "    return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxg9wkG3IdVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
        "    self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
        "    self.out = nn.Linear(d_model, trg_mask)\n",
        "    \n",
        "  def forward(self, src, trg, src_mask, trg_mask):\n",
        "    e_outputs = self.encoder(src, src_mask)\n",
        "    d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "    output = self.out(d_output)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chlhGBkHKVKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}